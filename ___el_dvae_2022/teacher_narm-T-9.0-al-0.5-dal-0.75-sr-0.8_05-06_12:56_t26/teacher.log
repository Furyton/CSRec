INFO    [05-06 12:56:54] setup_train in utils:
{
    "experiment_dir": "___el_dvae_2022",
    "experiment_description": "teacher",
    "describe": "{config[model_code]}-T-{config[T]}-al-{config[alpha]}-dal-{config[dvae_alpha]}-sr-{config[sample_rate]}",
    "dataset_name": "electronics.csv",
    "sched": "dvae",
    "model_code": "narm",
    "mentor_code": "narm",
    "mentor2_code": "narm",
    "training_routine": "teacher",
    "softmaxed_mentor": false,
    "dvae_alpha": 0.75,
    "alpha": 0.5,
    "T": 9.0,
    "enable_auto_path_finder": true,
    "sample_seed": 2000,
    "sample_rate": 0.8,
    "mentor_describe": "{config[model_code]}-rate-{config[sample_rate]}-seed-{config[sample_seed]}",
    "num_epochs": 80,
    "mode": [
        "train",
        "test"
    ],
    "max_len": 10,
    "model_state_path": null,
    "test_state_path": null,
    "mentor_state_path": "___el_base_partial_2022",
    "mentor2_state_path": null,
    "rand_seed": 2022,
    "load_processed_dataset": true,
    "save_processed_dataset": false,
    "dataset_cache_filename": null,
    "do_remap": false,
    "weight_decay": 0,
    "decay_step": 15,
    "gamma": 0.99,
    "lr": 0.001,
    "min_length": 5,
    "min_item_inter": 5,
    "good_only": false,
    "use_rating": true,
    "test_negative_sampler_code": "random",
    "test_negative_sample_size": 0,
    "dataloader_type": "next",
    "train_batch_size": 4096,
    "val_batch_size": 4096,
    "test_batch_size": 4096,
    "prop_sliding_window": -1.0,
    "worker_number": 2,
    "metric_ks": [
        5,
        10
    ],
    "device": "cuda",
    "num_gpu": 1,
    "optimizer": "Adam",
    "best_metric": "NDCG@10",
    "show_process_bar": false,
    "enable_sample": false,
    "samples_ratio": 0.1,
    "config_file": "config2.electronics/narm/config_dvae.json",
    "task_id": 26,
    "split": "leave_one_out",
    "do_sampling": false,
    "path_for_sample": null,
    "train_negative_sampler_code": "random",
    "train_negative_sample_size": 100,
    "device_idx": "0",
    "momentum": null,
    "log_period_as_iter": 12800,
    "weight_list": [
        0.5,
        0.5
    ],
    "validation_rate": 0.2,
    "num_items": null,
    "start_index": 1,
    "kwargs": null
}
DEBUG   [05-06 12:56:54] __init__ in DVAEDistillSched:
DVAEDistillScheduler attribs: auxiliary tag=auxiliary_narm, teacher tag=teacher_narm, student tag=student_narm
INFO    [05-06 12:56:54] _load_full_dataset_from_path in utils:
loading cache from /data01/wushiguang-slurm/Codes/soft-rec/Data/Cache/electronics-5-5.pkl
INFO    [05-06 12:56:56] _check_dataset_cache in utils:
check if the cache is generated under this configuration
INFO    [05-06 12:56:56] _check_dataset_cache in utils:
correct.
INFO    [05-06 12:56:56] __init__ in NextItemDataloader:
there are 29351 items in this dataset, 102187 users, padding_first? False
DEBUG   [05-06 12:57:11] generate_model in utils:
model_code_list=['narm']
INFO    [05-06 12:57:11] load_model_config in utils:
loading model narm's config file at asset/config/model/narm.json
INFO    [05-06 12:57:11] load_model_config in utils:
{
    "embedding_size": 64,
    "hidden_size": 128,
    "n_layers": 1,
    "dropout_probs": [
        0.25,
        0.5
    ]
}
WARNING [05-06 12:57:20] get_exist_path in utils:
dir ___el_dvae_2022/teacher_narm-T-9.0-al-0.5-dal-0.75-sr-0.8_05-06_12:56_t26/auxiliary_narm_logs does not exist! Create one.
WARNING [05-06 12:57:20] get_exist_path in utils:
dir ___el_dvae_2022/teacher_narm-T-9.0-al-0.5-dal-0.75-sr-0.8_05-06_12:56_t26/auxiliary_narm_logs/tb_vis does not exist! Create one.
WARNING [05-06 12:57:20] get_exist_path in utils:
dir ___el_dvae_2022/teacher_narm-T-9.0-al-0.5-dal-0.75-sr-0.8_05-06_12:56_t26/auxiliary_narm_logs/checkpoint does not exist! Create one.
INFO    [05-06 12:57:21] load_state_from_given_path in utils:
checkpoint epoch: 78
INFO    [05-06 12:57:21] load_state_from_given_path in utils:
Loading model's parameters
INFO    [05-06 12:57:21] load_state_from_given_path in utils:
Loading optimizer's parameters
DEBUG   [05-06 12:57:21] _generate_auxliary_trainer in DVAEDistillSched:
auxiliary model: 
NARM(
  (item_embedding): Embedding(29352, 64, padding_idx=0)
  (emb_dropout): Dropout(p=0.25, inplace=False)
  (gru): GRU(64, 128, bias=False, batch_first=True)
  (a_1): Linear(in_features=128, out_features=128, bias=False)
  (a_2): Linear(in_features=128, out_features=128, bias=False)
  (v_t): Linear(in_features=128, out_features=1, bias=False)
  (ct_dropout): Dropout(p=0.5, inplace=False)
  (b): Linear(in_features=256, out_features=64, bias=False)
  (loss_fct): CrossEntropyLoss()
)
INFO    [05-06 12:57:21] assert_model_device in utils:
model auxiliary_narm has 7.63525390625 MB params.
INFO    [05-06 12:57:21] __init__ in BasicTrainer:
454656 iter per epoch
DEBUG   [05-06 12:57:21] generate_model in utils:
model_code_list=['prior']
INFO    [05-06 12:57:21] load_model_config in utils:
loading model prior's config file at asset/config/model/prior.json
INFO    [05-06 12:57:21] load_model_config in utils:
{
    "hidden_size": 64
}
DEBUG   [05-06 12:57:25] generate_model in utils:
model_code_list=['narm']
INFO    [05-06 12:57:25] load_model_config in utils:
loading model narm's config file at asset/config/model/narm.json
INFO    [05-06 12:57:25] load_model_config in utils:
{
    "embedding_size": 64,
    "hidden_size": 128,
    "n_layers": 1,
    "dropout_probs": [
        0.25,
        0.5
    ]
}
WARNING [05-06 12:57:30] get_exist_path in utils:
dir ___el_dvae_2022/teacher_narm-T-9.0-al-0.5-dal-0.75-sr-0.8_05-06_12:56_t26/teacher_narm_logs does not exist! Create one.
WARNING [05-06 12:57:30] get_exist_path in utils:
dir ___el_dvae_2022/teacher_narm-T-9.0-al-0.5-dal-0.75-sr-0.8_05-06_12:56_t26/teacher_narm_logs/tb_vis does not exist! Create one.
WARNING [05-06 12:57:30] get_exist_path in utils:
dir ___el_dvae_2022/teacher_narm-T-9.0-al-0.5-dal-0.75-sr-0.8_05-06_12:56_t26/teacher_narm_logs/checkpoint does not exist! Create one.
WARNING [05-06 12:57:30] get_path in utils:
Path is None
WARNING [05-06 12:57:30] load_state_from_given_path in utils:
Not given any path.
DEBUG   [05-06 12:57:30] _generate_teacher_trainer in DVAEDistillSched:
prior model: 
PriorModel(
  (observed_embedding): Embedding(29352, 64, padding_idx=0)
  (prior_item_embedding): Embedding(29352, 64, padding_idx=0)
)
DEBUG   [05-06 12:57:30] _generate_teacher_trainer in DVAEDistillSched:
teacher model: 
NARM(
  (item_embedding): Embedding(29352, 64, padding_idx=0)
  (emb_dropout): Dropout(p=0.25, inplace=False)
  (gru): GRU(64, 128, bias=False, batch_first=True)
  (a_1): Linear(in_features=128, out_features=128, bias=False)
  (a_2): Linear(in_features=128, out_features=128, bias=False)
  (v_t): Linear(in_features=128, out_features=1, bias=False)
  (ct_dropout): Dropout(p=0.5, inplace=False)
  (b): Linear(in_features=256, out_features=64, bias=False)
  (loss_fct): CrossEntropyLoss()
)
INFO    [05-06 12:57:30] assert_model_device in utils:
model teacher_narm has 7.63525390625 MB params.
INFO    [05-06 12:57:30] assert_model_device in utils:
model auxiliary_narm has 7.63525390625 MB params.
INFO    [05-06 12:57:30] assert_model_device in utils:
model prior has 14.33203125 MB params.
INFO    [05-06 12:57:30] __init__ in DVAETrainer:
454656 iter per epoch
DEBUG   [05-06 12:57:30] generate_model in utils:
model_code_list=['narm']
INFO    [05-06 12:57:30] load_model_config in utils:
loading model narm's config file at asset/config/model/narm.json
INFO    [05-06 12:57:30] load_model_config in utils:
{
    "embedding_size": 64,
    "hidden_size": 128,
    "n_layers": 1,
    "dropout_probs": [
        0.25,
        0.5
    ]
}
WARNING [05-06 12:57:35] get_exist_path in utils:
dir ___el_dvae_2022/teacher_narm-T-9.0-al-0.5-dal-0.75-sr-0.8_05-06_12:56_t26/student_narm_logs does not exist! Create one.
WARNING [05-06 12:57:35] get_exist_path in utils:
dir ___el_dvae_2022/teacher_narm-T-9.0-al-0.5-dal-0.75-sr-0.8_05-06_12:56_t26/student_narm_logs/tb_vis does not exist! Create one.
WARNING [05-06 12:57:35] get_exist_path in utils:
dir ___el_dvae_2022/teacher_narm-T-9.0-al-0.5-dal-0.75-sr-0.8_05-06_12:56_t26/student_narm_logs/checkpoint does not exist! Create one.
WARNING [05-06 12:57:35] get_path in utils:
Path is None
WARNING [05-06 12:57:35] load_state_from_given_path in utils:
Not given any path.
DEBUG   [05-06 12:57:35] _genearte_student_trainer in DVAEDistillSched:
student model: 
NARM(
  (item_embedding): Embedding(29352, 64, padding_idx=0)
  (emb_dropout): Dropout(p=0.25, inplace=False)
  (gru): GRU(64, 128, bias=False, batch_first=True)
  (a_1): Linear(in_features=128, out_features=128, bias=False)
  (a_2): Linear(in_features=128, out_features=128, bias=False)
  (v_t): Linear(in_features=128, out_features=1, bias=False)
  (ct_dropout): Dropout(p=0.5, inplace=False)
  (b): Linear(in_features=256, out_features=64, bias=False)
  (loss_fct): CrossEntropyLoss()
)
INFO    [05-06 12:57:35] assert_model_device in utils:
model student_narm has 7.63525390625 MB params.
INFO    [05-06 12:57:35] assert_model_device in utils:
model teacher_narm has 7.63525390625 MB params.
INFO    [05-06 12:57:35] __init__ in DistillTrainer:
454656 iter per epoch
INFO    [05-06 12:57:35] _set_current_routine in Routine:
first routine: teacher
INFO    [05-06 12:57:35] run_routine in Routine:
Start routine teacher
INFO    [05-06 12:57:35] run_routine in Routine:
Start training
INFO    [05-06 12:58:17] log in loggers:
Update Best NDCG@10 Model at 0
INFO    [05-06 12:58:17] validate in BasicTrainer:
{'Recall@10': 0.000380859375, 'NDCG@10': 0.00016723612614441663, 'MRR@10': 0.00010360475571360439, 'Recall@5': 0.00021484375, 'NDCG@5': 0.00011410955397877842, 'MRR@5': 8.203125064028427e-05, 'Recall*@10': 0.00038453699904493987, 'NDCG*@10': 0.00016273027984425425, 'MRR*@10': 9.700098380562849e-05, 'Recall*@5': 0.00020425495691597461, 'NDCG*@5': 0.00010497452516574413, 'MRR*@5': 7.350673477048985e-05}
INFO    [05-06 12:58:17] train in BasicTrainer:
epoch: 0
DEBUG   [05-06 12:58:28] calculate_loss in loss:
KL_loss1: 
tensor(3.5700, device='cuda:0', grad_fn=<DivBackward0>)
DEBUG   [05-06 12:58:28] calculate_loss in loss:
KL_loss2: 
tensor(2.8348, device='cuda:0', grad_fn=<DivBackward0>)
DEBUG   [05-06 12:58:28] calculate_loss in loss:
expectation_loss: 
tensor(11.1756, device='cuda:0', grad_fn=<AddBackward0>)
DEBUG   [05-06 12:58:28] calculate_loss in loss:
max in g softmax: 0.9999115467071533 argmax: 10161078
DEBUG   [05-06 12:58:28] calculate_loss in loss:
max in f softmax: 0.07735227793455124 argmax: 4681243
DEBUG   [05-06 12:58:28] calculate_loss in loss:
max in h softmax: 4.3373482185415924e-05 argmax: 111684379
DEBUG   [05-06 12:58:28] calculate_loss in loss:
max_in_col: tensor([[0.2064, 0.2037, 0.1733,  ..., 0.2178, 0.1890, 0.1744],
        [0.2064, 0.2037, 0.1733,  ..., 0.2178, 0.1890, 0.1744],
        [0.2064, 0.2037, 0.1733,  ..., 0.2178, 0.1890, 0.1744],
        ...,
        [0.2064, 0.2037, 0.1733,  ..., 0.2178, 0.1890, 0.1744],
        [0.2064, 0.2037, 0.1733,  ..., 0.2178, 0.1890, 0.1744],
        [0.2064, 0.2037, 0.1733,  ..., 0.2178, 0.1890, 0.1744]],
       device='cuda:0', grad_fn=<SliceBackward0>)
DEBUG   [05-06 12:58:39] calculate_loss in loss:
KL_loss1: 
tensor(2.9060, device='cuda:0', grad_fn=<DivBackward0>)
DEBUG   [05-06 12:58:39] calculate_loss in loss:
KL_loss2: 
tensor(2.5713, device='cuda:0', grad_fn=<DivBackward0>)
DEBUG   [05-06 12:58:39] calculate_loss in loss:
expectation_loss: 
tensor(11.1606, device='cuda:0', grad_fn=<AddBackward0>)
DEBUG   [05-06 12:58:39] calculate_loss in loss:
max in g softmax: 0.9999523162841797 argmax: 26039698
DEBUG   [05-06 12:58:39] calculate_loss in loss:
max in f softmax: 0.045912936329841614 argmax: 32671595
DEBUG   [05-06 12:58:39] calculate_loss in loss:
max in h softmax: 4.421036646817811e-05 argmax: 65728509
DEBUG   [05-06 12:58:39] calculate_loss in loss:
max_in_col: tensor([[0.2064, 0.2037, 0.1812,  ..., 0.2178, 0.1987, 0.1744],
        [0.2064, 0.2037, 0.1812,  ..., 0.2178, 0.1987, 0.1744],
        [0.2064, 0.2037, 0.1812,  ..., 0.2178, 0.1987, 0.1744],
        ...,
        [0.2064, 0.2037, 0.1812,  ..., 0.2178, 0.1987, 0.1744],
        [0.2064, 0.2037, 0.1812,  ..., 0.2178, 0.1987, 0.1744],
        [0.2064, 0.2037, 0.1812,  ..., 0.2178, 0.1987, 0.1744]],
       device='cuda:0', grad_fn=<SliceBackward0>)
INFO    [05-06 12:58:41] _train_one_epoch in BasicTrainer:
loss = 24.624756632624447
INFO    [05-06 12:59:21] log in loggers:
Update Best NDCG@10 Model at 0
INFO    [05-06 12:59:22] validate in BasicTrainer:
{'Recall@10': 0.0324398846924305, 'NDCG@10': 0.016805587597191335, 'MRR@10': 0.012074335217475892, 'Recall@5': 0.020362319052219392, 'NDCG@5': 0.01295871526002884, 'MRR@5': 0.010520416460931301, 'Recall*@10': 0.03567421570420265, 'NDCG*@10': 0.0188762129470706, 'MRR*@10': 0.013777273409068584, 'Recall*@5': 0.02298210680484772, 'NDCG*@5': 0.014825227186083794, 'MRR*@5': 0.012136077284812927}
INFO    [05-06 12:59:22] train in BasicTrainer:
duration: 65.11215949058533s
INFO    [05-06 12:59:22] train in BasicTrainer:
epoch: 1
DEBUG   [05-06 12:59:31] calculate_loss in loss:
KL_loss1: 
tensor(2.7681, device='cuda:0', grad_fn=<DivBackward0>)
DEBUG   [05-06 12:59:31] calculate_loss in loss:
KL_loss2: 
tensor(2.4532, device='cuda:0', grad_fn=<DivBackward0>)
DEBUG   [05-06 12:59:31] calculate_loss in loss:
expectation_loss: 
tensor(11.1543, device='cuda:0', grad_fn=<AddBackward0>)
DEBUG   [05-06 12:59:31] calculate_loss in loss:
max in g softmax: 0.9997871518135071 argmax: 34898598
DEBUG   [05-06 12:59:31] calculate_loss in loss:
max in f softmax: 0.04023141413927078 argmax: 113015784
DEBUG   [05-06 12:59:31] calculate_loss in loss:
max in h softmax: 4.390537287690677e-05 argmax: 44449568
DEBUG   [05-06 12:59:31] calculate_loss in loss:
max_in_col: tensor([[0.2124, 0.2466, 0.1972,  ..., 0.2178, 0.2308, 0.2055],
        [0.2124, 0.2466, 0.1972,  ..., 0.2178, 0.2308, 0.2055],
        [0.2124, 0.2466, 0.1972,  ..., 0.2178, 0.2308, 0.2055],
        ...,
        [0.2124, 0.2466, 0.1972,  ..., 0.2178, 0.2308, 0.2055],
        [0.2124, 0.2466, 0.1972,  ..., 0.2178, 0.2308, 0.2055],
        [0.2124, 0.2466, 0.1972,  ..., 0.2178, 0.2308, 0.2055]],
       device='cuda:0', grad_fn=<SliceBackward0>)
DEBUG   [05-06 12:59:42] calculate_loss in loss:
KL_loss1: 
tensor(2.7352, device='cuda:0', grad_fn=<DivBackward0>)
DEBUG   [05-06 12:59:42] calculate_loss in loss:
KL_loss2: 
tensor(2.4654, device='cuda:0', grad_fn=<DivBackward0>)
DEBUG   [05-06 12:59:42] calculate_loss in loss:
expectation_loss: 
tensor(11.1624, device='cuda:0', grad_fn=<AddBackward0>)
DEBUG   [05-06 12:59:42] calculate_loss in loss:
max in g softmax: 0.9999984502792358 argmax: 39775116
DEBUG   [05-06 12:59:42] calculate_loss in loss:
max in f softmax: 0.7541384100914001 argmax: 22341002
DEBUG   [05-06 12:59:42] calculate_loss in loss:
max in h softmax: 4.364975029602647e-05 argmax: 116588640
DEBUG   [05-06 12:59:42] calculate_loss in loss:
max_in_col: tensor([[0.2374, 0.2943, 0.2291,  ..., 0.2397, 0.2643, 0.2379],
        [0.2374, 0.2943, 0.2291,  ..., 0.2397, 0.2643, 0.2379],
        [0.2374, 0.2943, 0.2291,  ..., 0.2397, 0.2643, 0.2379],
        ...,
        [0.2374, 0.2943, 0.2291,  ..., 0.2397, 0.2643, 0.2379],
        [0.2374, 0.2943, 0.2291,  ..., 0.2397, 0.2643, 0.2379],
        [0.2374, 0.2943, 0.2291,  ..., 0.2397, 0.2643, 0.2379]],
       device='cuda:0', grad_fn=<SliceBackward0>)
INFO    [05-06 12:59:47] _train_one_epoch in BasicTrainer:
loss = 23.346233625669736
INFO    [05-06 13:00:29] log in loggers:
Update Best NDCG@10 Model at 1
INFO    [05-06 13:00:30] validate in BasicTrainer:
{'Recall@10': 0.03527405872941017, 'NDCG@10': 0.018292123563587667, 'MRR@10': 0.01313541978597641, 'Recall@5': 0.022418992891907692, 'NDCG@5': 0.01417849276214838, 'MRR@5': 0.011462769769132138, 'Recall*@10': 0.038754656985402106, 'NDCG*@10': 0.020335901901125907, 'MRR*@10': 0.01473859865218401, 'Recall*@5': 0.024812388867139816, 'NDCG*@5': 0.015873249098658563, 'MRR*@5': 0.01292346216738224}
INFO    [05-06 13:00:30] train in BasicTrainer:
duration: 67.69484305381775s
INFO    [05-06 13:00:30] train in BasicTrainer:
epoch: 2
DEBUG   [05-06 13:00:37] calculate_loss in loss:
KL_loss1: 
tensor(2.5865, device='cuda:0', grad_fn=<DivBackward0>)
DEBUG   [05-06 13:00:37] calculate_loss in loss:
KL_loss2: 
tensor(2.4006, device='cuda:0', grad_fn=<DivBackward0>)
DEBUG   [05-06 13:00:37] calculate_loss in loss:
expectation_loss: 
tensor(11.1789, device='cuda:0', grad_fn=<AddBackward0>)
DEBUG   [05-06 13:00:37] calculate_loss in loss:
max in g softmax: 0.9998621940612793 argmax: 34155576
DEBUG   [05-06 13:00:37] calculate_loss in loss:
max in f softmax: 0.0645294338464737 argmax: 26554427
DEBUG   [05-06 13:00:37] calculate_loss in loss:
max in h softmax: 4.42469900008291e-05 argmax: 41978398
DEBUG   [05-06 13:00:37] calculate_loss in loss:
max_in_col: tensor([[0.2774, 0.3407, 0.2565,  ..., 0.2691, 0.3096, 0.2868],
        [0.2774, 0.3407, 0.2565,  ..., 0.2691, 0.3096, 0.2868],
        [0.2774, 0.3407, 0.2565,  ..., 0.2691, 0.3096, 0.2868],
        ...,
        [0.2774, 0.3407, 0.2565,  ..., 0.2691, 0.3096, 0.2868],
        [0.2774, 0.3407, 0.2565,  ..., 0.2691, 0.3096, 0.2868],
        [0.2774, 0.3407, 0.2565,  ..., 0.2691, 0.3096, 0.2868]],
       device='cuda:0', grad_fn=<SliceBackward0>)
DEBUG   [05-06 13:00:47] calculate_loss in loss:
KL_loss1: 
tensor(2.3217, device='cuda:0', grad_fn=<DivBackward0>)
DEBUG   [05-06 13:00:47] calculate_loss in loss:
KL_loss2: 
tensor(2.2315, device='cuda:0', grad_fn=<DivBackward0>)
DEBUG   [05-06 13:00:47] calculate_loss in loss:
expectation_loss: 
tensor(11.2050, device='cuda:0', grad_fn=<AddBackward0>)
DEBUG   [05-06 13:00:47] calculate_loss in loss:
max in g softmax: 0.9999370574951172 argmax: 51490633
DEBUG   [05-06 13:00:47] calculate_loss in loss:
max in f softmax: 0.26385822892189026 argmax: 75992321
DEBUG   [05-06 13:00:47] calculate_loss in loss:
max in h softmax: 4.509738937485963e-05 argmax: 65659631
DEBUG   [05-06 13:00:47] calculate_loss in loss:
max_in_col: tensor([[0.3470, 0.3954, 0.3023,  ..., 0.3090, 0.3584, 0.3273],
        [0.3470, 0.3954, 0.3023,  ..., 0.3090, 0.3584, 0.3273],
        [0.3470, 0.3954, 0.3023,  ..., 0.3090, 0.3584, 0.3273],
        ...,
        [0.3470, 0.3954, 0.3023,  ..., 0.3090, 0.3584, 0.3273],
        [0.3470, 0.3954, 0.3023,  ..., 0.3090, 0.3584, 0.3273],
        [0.3470, 0.3954, 0.3023,  ..., 0.3090, 0.3584, 0.3273]],
       device='cuda:0', grad_fn=<SliceBackward0>)
INFO    [05-06 13:00:54] _train_one_epoch in BasicTrainer:
loss = 22.892650209031665
INFO    [05-06 13:01:35] log in loggers:
Update Best NDCG@10 Model at 2
INFO    [05-06 13:01:36] validate in BasicTrainer:
{'Recall@10': 0.03900720596313476, 'NDCG@10': 0.02100567616522312, 'MRR@10': 0.015522515028715133, 'Recall@5': 0.026082173585891724, 'NDCG@5': 0.016868550069630145, 'MRR@5': 0.013840125575661659, 'Recall*@10': 0.042631921619176866, 'NDCG*@10': 0.023009384348988533, 'MRR*@10': 0.017026596479117872, 'Recall*@5': 0.028684910833835602, 'NDCG*@5': 0.01853939823806286, 'MRR*@5': 0.015205482915043832}
INFO    [05-06 13:01:36] train in BasicTrainer:
duration: 65.97817802429199s
INFO    [05-06 13:01:36] train in BasicTrainer:
epoch: 3
DEBUG   [05-06 13:01:40] calculate_loss in loss:
KL_loss1: 
tensor(2.1923, device='cuda:0', grad_fn=<DivBackward0>)
DEBUG   [05-06 13:01:40] calculate_loss in loss:
KL_loss2: 
tensor(2.1105, device='cuda:0', grad_fn=<DivBackward0>)
DEBUG   [05-06 13:01:40] calculate_loss in loss:
expectation_loss: 
tensor(11.2367, device='cuda:0', grad_fn=<AddBackward0>)
DEBUG   [05-06 13:01:40] calculate_loss in loss:
max in g softmax: 0.999983549118042 argmax: 68243688
DEBUG   [05-06 13:01:40] calculate_loss in loss:
max in f softmax: 0.19663968682289124 argmax: 50850211
DEBUG   [05-06 13:01:40] calculate_loss in loss:
max in h softmax: 4.59440634585917e-05 argmax: 92222286
DEBUG   [05-06 13:01:40] calculate_loss in loss:
max_in_col: tensor([[0.4352, 0.4729, 0.3476,  ..., 0.3826, 0.4199, 0.3828],
        [0.4352, 0.4729, 0.3476,  ..., 0.3826, 0.4199, 0.3828],
        [0.4352, 0.4729, 0.3476,  ..., 0.3826, 0.4199, 0.3828],
        ...,
        [0.4352, 0.4729, 0.3476,  ..., 0.3826, 0.4199, 0.3828],
        [0.4352, 0.4729, 0.3476,  ..., 0.3826, 0.4199, 0.3828],
        [0.4352, 0.4729, 0.3476,  ..., 0.3826, 0.4199, 0.3828]],
       device='cuda:0', grad_fn=<SliceBackward0>)
