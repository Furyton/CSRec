INFO    [05-05 22:44:24] setup_train in utils:
{
    "experiment_dir": "___el_soft_2022",
    "experiment_description": "pop",
    "describe": "{config[model_code]}-T-{config[T]}-al-{config[alpha]}",
    "dataset_name": "electronics.csv",
    "sched": "distill",
    "model_code": "narm",
    "mentor_code": "pop",
    "training_routine": "student",
    "softmaxed_mentor": false,
    "alpha": 0.5,
    "T": 6.0,
    "num_epochs": 60,
    "mode": [
        "train",
        "test"
    ],
    "max_len": 10,
    "test_state_path": null,
    "model_state_path": null,
    "mentor_state_path": null,
    "rand_seed": 2022,
    "load_processed_dataset": true,
    "save_processed_dataset": false,
    "dataset_cache_filename": null,
    "do_remap": false,
    "weight_decay": 0,
    "decay_step": 15,
    "gamma": 0.99,
    "lr": 0.001,
    "min_length": 5,
    "min_item_inter": 5,
    "good_only": false,
    "use_rating": true,
    "test_negative_sampler_code": "random",
    "test_negative_sample_size": 0,
    "dataloader_type": "next",
    "train_batch_size": 4096,
    "val_batch_size": 4096,
    "test_batch_size": 4096,
    "prop_sliding_window": -1.0,
    "worker_number": 2,
    "metric_ks": [
        5,
        10
    ],
    "device": "cuda",
    "num_gpu": 1,
    "optimizer": "Adam",
    "best_metric": "NDCG@10",
    "show_process_bar": false,
    "enable_sample": false,
    "samples_ratio": 0.1,
    "config_file": "config2.electronics/narm/config_distill.json",
    "task_id": 8,
    "split": "leave_one_out",
    "do_sampling": false,
    "path_for_sample": null,
    "sample_rate": 0.5,
    "sample_seed": 0,
    "train_negative_sampler_code": "random",
    "train_negative_sample_size": 100,
    "device_idx": "0",
    "momentum": null,
    "log_period_as_iter": 12800,
    "dvae_alpha": 0.5,
    "weight_list": [
        0.5,
        0.5
    ],
    "validation_rate": 0.2,
    "num_items": null,
    "start_index": 1,
    "kwargs": null
}
DEBUG   [05-05 22:44:24] __init__ in DistillSched:
DistillScheduler attribs: teacher tag=teacher_pop, student tag=student_narm
INFO    [05-05 22:44:24] _load_full_dataset_from_path in utils:
loading cache from /data01/wushiguang-slurm/Codes/soft-rec/Data/Cache/electronics-5-5.pkl
INFO    [05-05 22:44:25] _check_dataset_cache in utils:
check if the cache is generated under this configuration
INFO    [05-05 22:44:25] _check_dataset_cache in utils:
correct.
INFO    [05-05 22:44:25] __init__ in NextItemDataloader:
there are 29351 items in this dataset, 102187 users, padding_first? False
DEBUG   [05-05 22:44:37] generate_model in utils:
model_code_list=['pop']
INFO    [05-05 22:44:37] load_model_config in utils:
loading model pop's config file at asset/config/model/pop.json
INFO    [05-05 22:44:37] load_model_config in utils:
{}
WARNING [05-05 22:45:32] get_exist_path in utils:
dir ___el_soft_2022/pop_narm-T-6.0-al-0.5_05-05_22:44_t8/teacher_pop_logs does not exist! Create one.
WARNING [05-05 22:45:33] get_exist_path in utils:
dir ___el_soft_2022/pop_narm-T-6.0-al-0.5_05-05_22:44_t8/teacher_pop_logs/tb_vis does not exist! Create one.
WARNING [05-05 22:45:35] get_exist_path in utils:
dir ___el_soft_2022/pop_narm-T-6.0-al-0.5_05-05_22:44_t8/teacher_pop_logs/checkpoint does not exist! Create one.
WARNING [05-05 22:45:35] get_path in utils:
Path is None
WARNING [05-05 22:45:35] load_state_from_given_path in utils:
Not given any path.
DEBUG   [05-05 22:45:35] __init__ in DistillSched:
teacher model: 
POPModel(
  (fake_parameter): Linear(in_features=1, out_features=1, bias=False)
)
INFO    [05-05 22:45:35] assert_model_device in utils:
model teacher_pop has 3.814697265625e-06 MB params.
INFO    [05-05 22:45:35] __init__ in BasicTrainer:
454656 iter per epoch
DEBUG   [05-05 22:45:35] generate_model in utils:
model_code_list=['narm']
INFO    [05-05 22:45:35] load_model_config in utils:
loading model narm's config file at asset/config/model/narm.json
INFO    [05-05 22:45:36] load_model_config in utils:
{
    "embedding_size": 64,
    "hidden_size": 128,
    "n_layers": 1,
    "dropout_probs": [
        0.25,
        0.5
    ]
}
WARNING [05-05 22:46:26] get_exist_path in utils:
dir ___el_soft_2022/pop_narm-T-6.0-al-0.5_05-05_22:44_t8/student_narm_logs does not exist! Create one.
WARNING [05-05 22:46:26] get_exist_path in utils:
dir ___el_soft_2022/pop_narm-T-6.0-al-0.5_05-05_22:44_t8/student_narm_logs/tb_vis does not exist! Create one.
WARNING [05-05 22:46:26] get_exist_path in utils:
dir ___el_soft_2022/pop_narm-T-6.0-al-0.5_05-05_22:44_t8/student_narm_logs/checkpoint does not exist! Create one.
WARNING [05-05 22:46:26] get_path in utils:
Path is None
WARNING [05-05 22:46:26] load_state_from_given_path in utils:
Not given any path.
DEBUG   [05-05 22:46:26] __init__ in DistillSched:
student model: 
NARM(
  (item_embedding): Embedding(29352, 64, padding_idx=0)
  (emb_dropout): Dropout(p=0.25, inplace=False)
  (gru): GRU(64, 128, bias=False, batch_first=True)
  (a_1): Linear(in_features=128, out_features=128, bias=False)
  (a_2): Linear(in_features=128, out_features=128, bias=False)
  (v_t): Linear(in_features=128, out_features=1, bias=False)
  (ct_dropout): Dropout(p=0.5, inplace=False)
  (b): Linear(in_features=256, out_features=64, bias=False)
  (loss_fct): CrossEntropyLoss()
)
INFO    [05-05 22:46:26] assert_model_device in utils:
model student_narm has 7.63525390625 MB params.
INFO    [05-05 22:46:26] assert_model_device in utils:
model teacher_pop has 3.814697265625e-06 MB params.
INFO    [05-05 22:46:26] __init__ in DistillTrainer:
454656 iter per epoch
INFO    [05-05 22:46:26] _set_current_routine in Routine:
first routine: student
INFO    [05-05 22:46:26] run_routine in Routine:
Start routine student
INFO    [05-05 22:46:26] run_routine in Routine:
Start training
INFO    [05-05 22:46:26] train in DistillTrainer:
Test mentor model: teacher_pop
